### copyright 2026 jixiangluo    ###
### email:jixiangluo85@gmail.com ###
### rights reserved by author    ###
### time: 2026-02-01             ###
### license: MIT                ###

"""
Rule-based Agent Executor for ChatPresenter

Provides reliable, rule-based task execution without LLM planning dependency.
Includes screen capture and visual understanding capabilities.
"""

import time
from typing import Dict, Any, List
import requests
import os
from datetime import datetime
from PIL import ImageGrab


class AgentExecutor:
    """
    Rule-based agent that executes common task workflows.

    This is simpler and more reliable than LLM-based planning.
    """

    def __init__(self, memory_system, ollama_base_url: str, current_model: str):
        """
        Initialize the agent executor.

        Args:
            memory_system: Memory system for searching
            ollama_base_url: Ollama API URL
            current_model: Current AI model to use
        """
        self.memory_system = memory_system
        self.ollama_base_url = ollama_base_url
        self.current_model = current_model
        self.vision_model = "qwen3:1.7b"  # Vision model for screen understanding

        # Create temp directory for screenshots
        self.temp_dir = "./db/temp"
        os.makedirs(self.temp_dir, exist_ok=True)

    def execute_task(self, user_message: str) -> Dict[str, Any]:
        """
        Execute a task based on user message.

        Args:
            user_message: User's task description

        Returns:
            Execution result with report
        """
        start_time = time.time()

        print(f"[AgentExecutor] ğŸ¤– Executing task: {user_message}")

        # Determine workflow
        workflow = self._analyze_task(user_message)

        # Execute workflow
        results = []
        for step in workflow["steps"]:
            print(f"[AgentExecutor] ğŸ“ Step: {step['description']}")
            result = self._execute_step(step, user_message, results)
            results.append(result)

        # Build response
        response = self._build_response(workflow, results, start_time)

        return {
            "success": True,
            "response": response,
            "execution_time": time.time() - start_time
        }

    def _analyze_task(self, user_message: str) -> Dict[str, Any]:
        """Analyze task and determine workflow."""
        user_msg_lower = user_message.lower()

        # Screen analysis (current screen)
        if any(kw in user_msg_lower for kw in ["å±å¹•ä¸Š", "ç°åœ¨å±å¹•", "å½“å‰å±å¹•", "æˆªå±", "screenshot", "what's on screen", "å±å¹•æœ‰ä»€ä¹ˆ", "çœ‹çœ‹å±å¹•"]):
            return {
                "type": "screen_analysis",
                "description": "å±å¹•åˆ†æ",
                "steps": [
                    {"type": "capture_screen", "description": "æ•è·å¹¶åˆ†æå½“å‰å±å¹•"},
                    {"type": "format", "description": "å±•ç¤ºç»“æœ"}
                ]
            }

        # Report generation
        if any(kw in user_msg_lower for kw in ["æŠ¥å‘Š", "ç”ŸæˆæŠ¥å‘Š", "å½¢æˆæŠ¥å‘Š", "report"]):
            return {
                "type": "report",
                "description": "æŠ¥å‘Šç”Ÿæˆ",
                "steps": [
                    {"type": "search", "description": "æœç´¢ç›¸å…³å±å¹•è®°å½•"},
                    {"type": "summarize", "description": "ç”Ÿæˆå†…å®¹æ‘˜è¦"},
                    {"type": "format", "description": "æ ¼å¼åŒ–æŠ¥å‘Š"}
                ]
            }

        # Summary task
        elif any(kw in user_msg_lower for kw in ["æ€»ç»“", "æ±‡æ€»", "summary"]):
            return {
                "type": "summary",
                "description": "å†…å®¹æ€»ç»“",
                "steps": [
                    {"type": "search", "description": "æœç´¢ç›¸å…³å†…å®¹"},
                    {"type": "summarize", "description": "ç”Ÿæˆæ‘˜è¦"}
                ]
            }

        # Search and process
        elif any(kw in user_msg_lower for kw in ["æœç´¢", "æŸ¥æ‰¾"]) and \
             any(kw in user_msg_lower for kw in ["å¹¶", "ç„¶å", "and", "then"]):
            return {
                "type": "search_and_process",
                "description": "æœç´¢ä¸å¤„ç†",
                "steps": [
                    {"type": "search", "description": "æ‰§è¡Œæœç´¢"},
                    {"type": "summarize", "description": "å¤„ç†ç»“æœ"}
                ]
            }

        # Analysis task
        elif any(kw in user_msg_lower for kw in ["åˆ†æ", "æµç¨‹", "æ¨¡å¼", "analyze", "workflow"]):
            return {
                "type": "analysis",
                "description": "æ•°æ®åˆ†æ",
                "steps": [
                    {"type": "search", "description": "æ”¶é›†æ•°æ®"},
                    {"type": "summarize", "description": "ç”Ÿæˆåˆ†æ"}
                ]
            }

        # Default: search only
        return {
            "type": "search",
            "description": "å†…å®¹æœç´¢",
            "steps": [
                {"type": "search", "description": "æœç´¢è®°å½•"},
                {"type": "format", "description": "å±•ç¤ºç»“æœ"}
            ]
        }

    def _execute_step(self, step: Dict, user_message: str, previous_results: List[Dict]) -> Dict[str, Any]:
        """Execute a single step."""
        step_type = step["type"]

        if step_type == "search":
            return self._execute_search(user_message)
        elif step_type == "capture_screen":
            return self._execute_capture_screen(user_message)
        elif step_type == "summarize":
            return self._execute_summarize(user_message, previous_results)
        elif step_type == "format":
            return {"success": True, "formatted": True}
        else:
            return {"success": False, "error": f"Unknown step type: {step_type}"}

    def _execute_search(self, query: str) -> Dict[str, Any]:
        """Execute search step."""
        try:
            if not self.memory_system:
                return {"success": False, "error": "Memory system not available"}

            print(f"[AgentExecutor] ğŸ” Searching: {query}")

            results = self.memory_system.search(
                query=query,
                user_id="screenshot"
            )

            if not results or 'results' not in results:
                return {"success": True, "count": 0, "results": []}

            search_results = results['results']
            print(f"[AgentExecutor] ğŸ” Found {len(search_results)} results")

            return {
                "success": True,
                "count": len(search_results),
                "results": search_results
            }

        except Exception as e:
            print(f"[AgentExecutor] âŒ Search error: {e}")
            return {"success": False, "error": str(e)}

    def _execute_capture_screen(self, query: str) -> Dict[str, Any]:
        """Execute screen capture and analysis step."""
        try:
            print(f"[AgentExecutor] ğŸ“¸ Capturing screen...")

            # Capture screen
            screenshot = ImageGrab.grab()

            # Save to temporary file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_path = os.path.join(self.temp_dir, f"temp_screenshot_{timestamp}.png")
            screenshot.save(temp_path)

            print(f"[AgentExecutor] ğŸ“¸ Screenshot saved: {temp_path}")

            # Analyze with vision model
            print(f"[AgentExecutor] ğŸ‘ï¸ Analyzing with vision model...")

            # Build prompt for vision understanding
            vision_prompt = f"""è¯·è¯¦ç»†æè¿°ä½ åœ¨è¿™ä¸ªæˆªå›¾ä¸­çœ‹åˆ°çš„å†…å®¹ã€‚åŒ…æ‹¬ï¼š
1. ä¸»è¦åº”ç”¨ç¨‹åºæˆ–çª—å£
2. ç•Œé¢å¸ƒå±€å’Œå…ƒç´ 
3. æ–‡æœ¬å†…å®¹ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
4. å›¾è¡¨ã€å›¾ç‰‡æˆ–æ•°æ®
5. ä»»ä½•æ˜¾è‘—çš„ç‰¹å¾æˆ–æ´»åŠ¨

ç”¨æˆ·çš„é—®é¢˜ï¼š{query}"""

            try:
                # Use Ollama vision API
                with open(temp_path, "rb") as image_file:
                    import base64
                    image_data = base64.b64encode(image_file.read()).decode('utf-8')

                response = requests.post(
                    f"{self.ollama_base_url}/api/generate",
                    json={
                        "model": self.vision_model,
                        "prompt": vision_prompt,
                        "images": [image_data],
                        "stream": False,
                        "options": {
                            "temperature": 0.7,
                            "num_predict": 500,
                            "top_p": 0.9,
                            "top_k": 40
                        }
                    },
                    timeout=90
                )

                if response.status_code == 200:
                    data = response.json()
                    analysis = data.get("response", "").strip()

                    print(f"[AgentExecutor] ğŸ‘ï¸ Vision analysis: {len(analysis)} chars")

                    # Clean up temp file
                    try:
                        os.remove(temp_path)
                        print(f"[AgentExecutor] ğŸ§¹ Cleaned up temp file")
                    except:
                        pass

                    return {
                        "success": True,
                        "analysis": analysis,
                        "screenshot_path": temp_path,
                        "type": "screen_capture"
                    }
                else:
                    raise Exception(f"Vision API error: {response.status_code}")

            except Exception as e:
                print(f"[AgentExecutor] âš ï¸ Vision model error: {e}")

                # Fallback: Try OCR extraction
                try:
                    import pytesseract
                    text = pytesseract.image_to_string(screenshot, lang='chi_sim+eng')

                    print(f"[AgentExecutor] ğŸ“„ OCR fallback: {len(text)} chars")

                    return {
                        "success": True,
                        "analysis": f"é€šè¿‡OCRæå–çš„å±å¹•æ–‡æœ¬å†…å®¹ï¼š\n\n{text[:1000]}",
                        "screenshot_path": temp_path,
                        "type": "ocr_fallback"
                    }
                except:
                    # Final fallback
                    return {
                        "success": True,
                        "analysis": "å·²æ•è·å±å¹•æˆªå›¾ï¼Œä½†è§†è§‰åˆ†ææš‚æ—¶ä¸å¯ç”¨ã€‚æˆªå›¾å·²ä¿å­˜ã€‚",
                        "screenshot_path": temp_path,
                        "type": "capture_only"
                    }

        except Exception as e:
            print(f"[AgentExecutor] âŒ Screen capture error: {e}")
            return {"success": False, "error": str(e)}

    def _execute_summarize(self, query: str, previous_results: List[Dict]) -> Dict[str, Any]:
        """Execute summarization step."""
        try:
            # Collect content from previous searches
            content_items = []

            for result in previous_results:
                if result.get("success") and result.get("results"):
                    for item in result["results"][:3]:  # Top 3
                        if isinstance(item, dict):
                            content = item.get("content", "")
                            metadata = item.get("metadata", {})
                            content_items.append({
                                "content": content[:400],
                                "type": metadata.get("type", "unknown"),
                                "timestamp": metadata.get("timestamp", "")
                            })

            if not content_items:
                return {
                    "success": True,
                    "summary": "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚å»ºè®®å…ˆå½•åˆ¶ä¸€äº›å±å¹•å†…å®¹ï¼Œç„¶åå†å°è¯•æŸ¥è¯¢ã€‚"
                }

            # Build content for summarization
            combined_content = "\n\n".join([
                f"[{item['type']}] {item['timestamp']}\n{item['content']}"
                for item in content_items
            ])

            print(f"[AgentExecutor] ğŸ“ Summarizing {len(content_items)} items")

            # Generate summary using LLM
            try:
                summary_prompt = f"""è¯·ç®€æ´æ€»ç»“ä»¥ä¸‹å±å¹•è®°å½•å†…å®¹ï¼ˆä¸è¶…è¿‡150å­—ï¼‰ï¼š

{combined_content}

æ€»ç»“ï¼š"""

                response = requests.post(
                    f"{self.ollama_base_url}/api/generate",
                    json={
                        "model": self.current_model,
                        "prompt": summary_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.6,
                            "num_predict": 250,
                            "top_p": 0.9,
                            "top_k": 30
                        }
                    },
                    timeout=60
                )

                if response.status_code == 200:
                    data = response.json()
                    summary = data.get("response", "").strip()
                    print(f"[AgentExecutor] ğŸ“ Summary: {len(summary)} chars")
                    return {"success": True, "summary": summary}
                else:
                    raise Exception(f"API error: {response.status_code}")

            except Exception as e:
                print(f"[AgentExecutor] âš ï¸ LLM error: {e}")
                # Fallback
                summary = f"æ‰¾åˆ° {len(content_items)} æ¡è®°å½•ï¼ŒåŒ…æ‹¬ï¼š{', '.join(set(item['type'] for item in content_items))}ã€‚"
                return {"success": True, "summary": summary}

        except Exception as e:
            print(f"[AgentExecutor] âŒ Summarization error: {e}")
            return {"success": False, "error": str(e)}

    def _build_response(self, workflow: Dict, results: List[Dict], start_time: float) -> str:
        """Build formatted response."""
        parts = []

        # Header
        parts.append(f"ğŸ¤– **AI Agent {workflow['description']}æŠ¥å‘Š**\n")

        # Steps and results
        for i, (step, result) in enumerate(zip(workflow["steps"], results), 1):
            parts.append(f"â³ æ­¥éª¤ {i}: {step['description']}")

            if result.get("success"):
                if "count" in result:
                    count = result["count"]
                    parts.append(f"âœ… å®Œæˆï¼šæ‰¾åˆ° {count} æ¡è®°å½•")

                    # Show top results
                    if result.get("results") and len(result["results"]) > 0:
                        parts.append(f"\nğŸ“Œ **æœ€ç›¸å…³ç»“æœ**:")
                        for j, item in enumerate(result["results"][:3], 1):
                            if isinstance(item, dict):
                                content = item.get("content", "")[:120]
                                score = item.get("score", 0)
                                parts.append(f"   {j}. [{score:.2f}] {content}...")
                        parts.append("")

                elif "analysis" in result:
                    analysis = result["analysis"]
                    result_type = result.get("type", "unknown")

                    if result_type == "screen_capture":
                        parts.append(f"[OK] å®Œæˆ\n")
                        parts.append(f"[Eye] **å±å¹•è§†è§‰åˆ†æ**:\n{analysis}\n")
                    elif result_type == "ocr_fallback":
                        parts.append(f"[OK] å®Œæˆï¼ˆOCRæ¨¡å¼ï¼‰\n")
                        parts.append(f"[Doc] **æ–‡æœ¬æå–**:\n{analysis}\n")
                    else:
                        parts.append(f"[OK] å®Œæˆ\n")
                        parts.append(f"[Chart] **åˆ†æç»“æœ**:\n{analysis}\n")

                elif "summary" in result:
                    summary = result["summary"]
                    parts.append(f"[OK] å®Œæˆ\n")
                    parts.append(f"[Note] **æ‘˜è¦**:\n{summary}\n")
                else:
                    parts.append("[OK] å®Œæˆ\n")
            else:
                error = result.get("error", "Unknown error")
                parts.append(f"âœ— å¤±è´¥: {error}\n")

        # Execution time
        exec_time = time.time() - start_time
        parts.append(f"\n[Time] æ‰§è¡Œæ—¶é—´: {exec_time:.2f} ç§’")

        return "\n".join(parts)


__all__ = ["AgentExecutor"]
