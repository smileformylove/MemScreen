### copyright 2026 jixiangluo    ###
### email:jixiangluo85@gmail.com ###
### rights reserved by author    ###
### time: 2026-01-29             ###
### license: MIT                ###

"""
Improved Agent Executor with Local Model Limitations Handling

This version is optimized for small local models (3B parameters):
- Simpler prompts
- Text-based formats instead of JSON
- Robust error handling
- Token budget management
- Fallback mechanisms
"""

import time
from typing import Dict, Any, List
import requests
import os
from datetime import datetime
from PIL import ImageGrab


class LocalModelExecutor:
    """
    Optimized agent executor for local models with limited capabilities.

    Key optimizations:
    1. Use simple text prompts instead of complex JSON
    2. Limit response length to avoid truncation
    3. Implement multiple fallback strategies
    4. Handle errors gracefully
    5. Work within token budget constraints
    """

    def __init__(self, memory_system, ollama_base_url: str, current_model: str):
        """
        Initialize the optimized executor.

        Args:
            memory_system: Memory system for searching
            ollama_base_url: Ollama API URL
            current_model: Current AI model to use
        """
        self.memory_system = memory_system
        self.ollama_base_url = ollama_base_url
        self.current_model = current_model
        self.vision_model = "qwen2.5vl:3b"

        # Model-specific constraints (optimized for speed and quality)
        self.max_tokens = 384  # Optimized for faster responses
        self.context_window = 4096  # Conservative context window
        self.temperature = 0.4  # Lower temperature for faster, focused responses

        # Create temp directory
        self.temp_dir = "./db/temp"
        os.makedirs(self.temp_dir, exist_ok=True)

    def execute_task(self, user_message: str) -> Dict[str, Any]:
        """
        Execute a task with robust error handling.

        Args:
            user_message: User's task description

        Returns:
            Execution result with comprehensive error handling
        """
        start_time = time.time()

        print(f"[LocalModelExecutor] ğŸ¤– Executing: {user_message[:50]}...")

        try:
            # Determine workflow
            workflow = self._analyze_task(user_message)

            # Execute workflow with error handling
            results = []
            for i, step in enumerate(workflow["steps"]):
                print(f"[LocalModelExecutor] ğŸ“ Step {i+1}/{len(workflow['steps'])}: {step['description']}")

                try:
                    result = self._execute_step(step, user_message, results)
                    results.append(result)

                    # Check for critical failures
                    if not result.get("success") and result.get("critical", False):
                        print(f"[LocalModelExecutor] âš ï¸ Critical failure in step {i+1}, stopping")
                        break

                except Exception as e:
                    print(f"[LocalModelExecutor] âŒ Error in step {i+1}: {e}")
                    # Add error result and continue
                    results.append({
                        "success": False,
                        "error": str(e),
                        "step": step['description']
                    })

            # Build response
            response = self._build_response(workflow, results, start_time)

            return {
                "success": True,
                "response": response,
                "execution_time": time.time() - start_time,
                "workflow_type": workflow["type"]
            }

        except Exception as e:
            print(f"[LocalModelExecutor] âŒ Fatal error: {e}")
            import traceback
            traceback.print_exc()

            # Return fallback response
            return {
                "success": False,
                "response": f"I encountered an error while processing your request: {str(e)}",
                "execution_time": time.time() - start_time,
                "error": str(e)
            }

    def _analyze_task(self, user_message: str) -> Dict[str, Any]:
        """Analyze task using simple keyword matching (no LLM needed)."""
        user_msg_lower = user_message.lower()

        # Screen analysis (highest priority)
        screen_keywords = ["å±å¹•ä¸Š", "ç°åœ¨å±å¹•", "å½“å‰å±å¹•", "æˆªå±", "screenshot", "what's on screen", "å±å¹•æœ‰ä»€ä¹ˆ", "çœ‹çœ‹å±å¹•"]
        if any(kw in user_msg_lower for kw in screen_keywords):
            return {
                "type": "screen_analysis",
                "description": "å±å¹•åˆ†æ",
                "steps": [
                    {"type": "capture_screen", "description": "æ•è·å¹¶åˆ†æå½“å‰å±å¹•"},
                    {"type": "format", "description": "å±•ç¤ºç»“æœ"}
                ]
            }

        # Report generation (requires search + summary)
        report_keywords = ["æŠ¥å‘Š", "ç”ŸæˆæŠ¥å‘Š", "å½¢æˆæŠ¥å‘Š", "report", "æ€»ç»“æŠ¥å‘Š", "åˆ†ææŠ¥å‘Š"]
        if any(kw in user_msg_lower for kw in report_keywords):
            return {
                "type": "report",
                "description": "æŠ¥å‘Šç”Ÿæˆ",
                "steps": [
                    {"type": "search", "description": "æœç´¢ç›¸å…³å±å¹•è®°å½•"},
                    {"type": "summarize", "description": "ç”Ÿæˆå†…å®¹æ‘˜è¦"},
                    {"type": "format", "description": "æ ¼å¼åŒ–æŠ¥å‘Š"}
                ]
            }

        # Summary task
        summary_keywords = ["æ€»ç»“", "æ±‡æ€»", "summary", "æ¦‚æ‹¬"]
        if any(kw in user_msg_lower for kw in summary_keywords):
            return {
                "type": "summary",
                "description": "å†…å®¹æ€»ç»“",
                "steps": [
                    {"type": "search", "description": "æœç´¢ç›¸å…³å†…å®¹"},
                    {"type": "summarize", "description": "ç”Ÿæˆæ‘˜è¦"}
                ]
            }

        # Search and analyze
        if any(kw in user_msg_lower for kw in ["æœç´¢", "æŸ¥æ‰¾", "search"]):
            return {
                "type": "search_and_analyze",
                "description": "æœç´¢ä¸åˆ†æ",
                "steps": [
                    {"type": "search", "description": "æ‰§è¡Œæœç´¢"},
                    {"type": "summarize", "description": "åˆ†æç»“æœ"}
                ]
            }

        # Default: simple search
        return {
            "type": "search",
            "description": "å†…å®¹æœç´¢",
            "steps": [
                {"type": "search", "description": "æœç´¢è®°å½•"},
                {"type": "format", "description": "å±•ç¤ºç»“æœ"}
            ]
        }

    def _execute_step(self, step: Dict, user_message: str, previous_results: List[Dict]) -> Dict[str, Any]:
        """Execute a single step with error handling."""
        step_type = step["type"]

        try:
            if step_type == "search":
                return self._execute_search(user_message)
            elif step_type == "capture_screen":
                return self._execute_capture_screen(user_message)
            elif step_type == "summarize":
                return self._execute_summarize_simple(user_message, previous_results)
            elif step_type == "format":
                return {"success": True, "formatted": True}
            else:
                return {"success": False, "error": f"Unknown step type: {step_type}"}

        except Exception as e:
            print(f"[LocalModelExecutor] âš ï¸ Step error: {e}")
            return {"success": False, "error": str(e)}

    def _execute_search(self, query: str) -> Dict[str, Any]:
        """Execute search with error handling."""
        try:
            if not self.memory_system:
                return {"success": False, "error": "Memory system not available", "critical": True}

            print(f"[LocalModelExecutor] ğŸ” Searching: {query[:50]}...")

            results = self.memory_system.search(
                query=query,
                user_id="screenshot"
            )

            if not results or 'results' not in results:
                return {"success": True, "count": 0, "results": []}

            search_results = results['results']
            print(f"[LocalModelExecutor] ğŸ” Found {len(search_results)} results")

            return {
                "success": True,
                "count": len(search_results),
                "results": search_results[:5]  # Limit to top 5 for token budget
            }

        except Exception as e:
            print(f"[LocalModelExecutor] âŒ Search error: {e}")
            return {"success": False, "error": str(e)}

    def _execute_capture_screen(self, query: str) -> Dict[str, Any]:
        """Execute screen capture with optimized vision prompt."""
        try:
            print(f"[LocalModelExecutor] ğŸ“¸ Capturing screen...")

            # Capture screen
            screenshot = ImageGrab.grab()

            # Save to temporary file
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            temp_path = os.path.join(self.temp_dir, f"temp_screenshot_{timestamp}.png")
            screenshot.save(temp_path)

            print(f"[LocalModelExecutor] ğŸ“¸ Screenshot saved")

            # Use simpler, more focused prompt for small models
            vision_prompt = f"""è¯·ç®€æ´æè¿°å±å¹•å†…å®¹ï¼ˆä¸è¶…è¿‡200å­—ï¼‰ï¼š
1. ä¸»è¦åº”ç”¨
2. ç•Œé¢å…ƒç´ 
3. æ–‡æœ¬å†…å®¹

ç”¨æˆ·é—®é¢˜ï¼š{query}"""

            try:
                # Use Ollama vision API with conservative settings
                with open(temp_path, "rb") as image_file:
                    import base64
                    image_data = base64.b64encode(image_file.read()).decode('utf-8')

                response = requests.post(
                    f"{self.ollama_base_url}/api/generate",
                    json={
                        "model": self.vision_model,
                        "prompt": vision_prompt,
                        "images": [image_data],
                        "stream": False,
                        "options": {
                            "temperature": 0.6,
                            "num_predict": 300,  # Conservative limit
                            "top_p": 0.9,
                            "top_k": 30
                        }
                    },
                    timeout=90
                )

                if response.status_code == 200:
                    data = response.json()
                    analysis = data.get("response", "").strip()

                    print(f"[LocalModelExecutor] ğŸ‘ï¸ Analysis: {len(analysis)} chars")

                    # Clean up temp file
                    try:
                        os.remove(temp_path)
                    except:
                        pass

                    return {
                        "success": True,
                        "analysis": analysis[:500],  # Limit length
                        "type": "screen_capture"
                    }
                else:
                    raise Exception(f"Vision API error: {response.status_code}")

            except Exception as e:
                print(f"[LocalModelExecutor] âš ï¸ Vision error: {e}")

                # Fallback
                return {
                    "success": True,
                    "analysis": "å±å¹•å·²æ•è·ï¼Œä½†è§†è§‰åˆ†ææš‚æ—¶ä¸å¯ç”¨ã€‚",
                    "type": "capture_fallback"
                }

        except Exception as e:
            print(f"[LocalModelExecutor] âŒ Capture error: {e}")
            return {"success": False, "error": str(e)}

    def _execute_summarize_simple(self, query: str, previous_results: List[Dict]) -> Dict[str, Any]:
        """Execute summarization with simple text-based approach."""
        try:
            # Collect content from previous searches
            content_items = []

            for result in previous_results:
                if result.get("success") and result.get("results"):
                    for item in result["results"][:3]:  # Top 3 only
                        if isinstance(item, dict):
                            content = item.get("content", "")[:300]  # Truncate long content
                            content_items.append(content)

            if not content_items:
                return {
                    "success": True,
                    "summary": "æ²¡æœ‰æ‰¾åˆ°ç›¸å…³å†…å®¹ã€‚å»ºè®®å…ˆå½•åˆ¶ä¸€äº›å±å¹•å†…å®¹ï¼Œç„¶åå†å°è¯•æŸ¥è¯¢ã€‚"
                }

            # Build simple summary prompt (text-based, no JSON)
            combined_content = "\n".join([f"- {item}" for item in content_items])

            summary_prompt = f"""è¯·ç®€æ´æ€»ç»“ä»¥ä¸‹å†…å®¹ï¼ˆä¸è¶…è¿‡150å­—ï¼‰ï¼š

{combined_content}

æ€»ç»“ï¼š"""

            print(f"[LocalModelExecutor] ğŸ“ Summarizing {len(content_items)} items")

            try:
                response = requests.post(
                    f"{self.ollama_base_url}/api/generate",
                    json={
                        "model": self.current_model,
                        "prompt": summary_prompt,
                        "stream": False,
                        "options": {
                            "temperature": 0.6,
                            "num_predict": 200,  # Conservative
                            "top_p": 0.9,
                            "top_k": 30
                        }
                    },
                    timeout=60
                )

                if response.status_code == 200:
                    data = response.json()
                    summary = data.get("response", "").strip()

                    print(f"[LocalModelExecutor] ğŸ“ Summary: {len(summary)} chars")
                    return {"success": True, "summary": summary[:300]}
                else:
                    raise Exception(f"API error: {response.status_code}")

            except Exception as e:
                print(f"[LocalModelExecutor] âš ï¸ LLM error: {e}")

                # Simple fallback summary
                count = len(content_items)
                fallback_summary = f"æ‰¾åˆ° {count} æ¡ç›¸å…³è®°å½•ã€‚å†…å®¹åŒ…æ‹¬ï¼š{combined_content[:100]}..."
                return {"success": True, "summary": fallback_summary}

        except Exception as e:
            print(f"[LocalModelExecutor] âŒ Summarization error: {e}")
            return {"success": False, "error": str(e)}

    def _build_response(self, workflow: Dict, results: List[Dict], start_time: float) -> str:
        """Build clear, user-friendly response."""
        parts = []

        # Header
        parts.append(f"ğŸ¤– **AI åŠ©æ‰‹ - {workflow['description']}**\n")

        # Steps and results
        for i, (step, result) in enumerate(zip(workflow["steps"], results), 1):
            parts.append(f"â³ æ­¥éª¤ {i}: {step['description']}")

            if result.get("success"):
                if "count" in result:
                    count = result["count"]
                    parts.append(f"âœ… å®Œæˆï¼šæ‰¾åˆ° {count} æ¡è®°å½•")

                    # Show top results
                    if result.get("results") and len(result["results"]) > 0:
                        parts.append(f"\nğŸ“Œ **æœ€ç›¸å…³ç»“æœ**:")
                        for j, item in enumerate(result["results"][:3], 1):
                            if isinstance(item, dict):
                                content = item.get("content", "")[:100]
                                score = item.get("score", 0)
                                parts.append(f"   {j}. [{score:.2f}] {content}...")
                        parts.append("")

                elif "analysis" in result:
                    analysis = result["analysis"]
                    parts.append(f"âœ… å®Œæˆ\n")
                    parts.append(f"ğŸ‘ï¸ **åˆ†æç»“æœ**:\n{analysis}\n")

                elif "summary" in result:
                    summary = result["summary"]
                    parts.append(f"âœ… å®Œæˆ\n")
                    parts.append(f"ğŸ“ **æ‘˜è¦**:\n{summary}\n")
                else:
                    parts.append("âœ… å®Œæˆ\n")
            else:
                error = result.get("error", "Unknown error")
                parts.append(f"âš ï¸ è¯¥æ­¥éª¤é‡åˆ°é—®é¢˜: {error}\n")

        # Execution time
        exec_time = time.time() - start_time
        parts.append(f"\nâ±ï¸ æ‰§è¡Œæ—¶é—´: {exec_time:.1f} ç§’")

        return "\n".join(parts)


__all__ = ["LocalModelExecutor"]
