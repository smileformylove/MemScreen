#!/usr/bin/env python3
"""
Local Model Optimization Guide for MemScreen

Provides tips and tools for optimizing MemScreen with small local models.
"""

import sys
import os

# Add project root to path
sys.path.insert(0, os.path.dirname(__file__))


def print_header(title):
    """Print formatted header"""
    print("\n" + "=" * 70)
    print(f" {title}")
    print("=" * 70)


def print_section(title):
    """Print formatted section"""
    print(f"\nâ–¶ {title}")
    print("-" * 70)


def main():
    print_header("MemScreen v0.4.0 - Local Model Optimization Guide")

    print("""
ğŸ¯ This guide helps you get the most out of MemScreen with small local models (3B)

Current Model: qwen2.5vl:3b
- Parameters: 3 Billion
- Context Window: ~4K tokens
- Strengths: Vision understanding, simple tasks
- Limitations: Complex reasoning, long outputs
    """)

    print_section("1. Model Capabilities & Limitations")

    print("""
âœ… What it CAN do well:
   â€¢ Screen capture and visual analysis
   â€¢ Simple Q&A about screenshots
   â€¢ Basic text summarization (short texts)
   â€¢ Keyword-based search
   â€¢ Single-step tasks

âš ï¸  What it STRUGGLES with:
   â€¢ Complex multi-step reasoning
   â€¢ Long documents (>1000 words)
   â€¢ Precise JSON formatting
   â€¢ Nuanced understanding
   â€¢ Creative writing
    """)

    print_section("2. Best Practices for Local Models")

    print("""
ğŸ“ Keep prompts SIMPLE:
   âŒ "Analyze the screenshot, extract all text, identify UI components,
       categorize them by type, and format the output as JSON with the
       following structure: {...}"

   âœ… "Describe what you see on this screen in 2-3 sentences."

ğŸ“Š Limit content length:
   â€¢ Summarize: Use texts <500 words
   â€¢ Search: Focus on specific keywords
   â€¢ Analysis: Break complex tasks into smaller steps

ğŸ¯ Be specific and direct:
   âŒ "Tell me about the stuff on my screen from earlier"
   âœ… "Search for Python code from today's recordings"

â±ï¸  Manage expectations:
   â€¢ Response time: 10-60 seconds per request
   â€¢ Accuracy: 70-85% for simple tasks
   â€¢ Errors: May need 1-2 retries
    """)

    print_section("3. Recommended Workflows")

    print("""
ğŸ”´ SCREEN ANALYSIS:
   "çœ‹çœ‹å±å¹•ä¸Šæœ‰ä»€ä¹ˆ"
   "åˆ†æå½“å‰å±å¹•å†…å®¹"
   âœ… Works well with vision model

ğŸ“Š CONTENT SEARCH:
   "æœç´¢Pythonç›¸å…³å†…å®¹"
   "æŸ¥æ‰¾ä»Šå¤©å½•åˆ¶çš„ä»£ç "
   âœ… Reliable and fast

ğŸ“ SIMPLE SUMMARY:
   "æ€»ç»“ä»Šå¤©å½•åˆ¶çš„Pythonæ•™ç¨‹"
   "æ¦‚æ‹¬æœ€è¿‘çš„å·¥ä½œå†…å®¹"
   âœ… Works well for <10 items

âš ï¸  AVOID - Complex Reports:
   "ç”Ÿæˆè¿‡å»ä¸€å‘¨æ‰€æœ‰æ´»åŠ¨çš„è¯¦ç»†åˆ†ææŠ¥å‘Šï¼ŒåŒ…æ‹¬æ¯ä¸ªåº”ç”¨çš„
   ä½¿ç”¨æ—¶é—´ã€é¢‘ç‡ã€ä»¥åŠå·¥ä½œæ¨¡å¼åˆ†æ"
   âŒ Too complex for 3B model
    """)

    print_section("4. Token Budget Management")

    print("""
ğŸ“Š Token limits for qwen2.5vl:3b:
   â€¢ Input prompt: ~500 tokens max recommended
   â€¢ Output response: ~200-300 tokens max
   â€¢ Vision images: ~1000 tokens per image

ğŸ’¡ Optimization tips:
   1. Short prompts = faster responses
   2. Limit search results to top 3-5
   3. Break long tasks into multiple steps
   4. Use specific keywords instead of vague descriptions

ğŸ“ˆ Performance estimates:
   â€¢ Simple search: 5-15 seconds
   â€¢ Screen analysis: 30-60 seconds
   â€¢ Summary: 20-40 seconds
   â€¢ Complex multi-step: May timeout or fail
    """)

    print_section("5. Troubleshooting Common Issues")

    print("""
âŒ "Model gives incomplete/empty responses"
   â†’ Reduce num_predict in model options
   â†’ Simplify your prompt
   â†’ Break into smaller tasks

âŒ "JSON parsing errors"
   â†’ Use text-based formats instead
   â†’ Avoid complex structures
   â†’ Accept free-form responses

âŒ "Timeout errors"
   â†’ Check if Ollama is running: ollama list
   â†’ Reduce task complexity
   â†’ Increase timeout in agent config

âŒ "Poor understanding of screen"
   â†’ Ensure sufficient lighting/contrast
   â†’ Try different screenshots
   â†’ Use specific questions
    """)

    print_section("6. When to Upgrade Models")

    print("""
ğŸš€ Consider upgrading to larger models if:
   â€¢ You need complex reasoning
   â€¢ Working with long documents
   â€¢ Require precise formatting
   â€¢ Need higher accuracy (>90%)

ğŸ’ª Recommended models:
   â€¢ qwen2.5vl:7b (Better reasoning, more VRAM needed)
   â€¢ llama3.2:11b (Excellent text understanding)
   â€¢ mixtral:8x7b (Strong multilingual capabilities)

âš™ï¸  Hardware requirements:
   â€¢ 3B models: 8GB RAM + 4GB VRAM
   â€¢ 7B models: 16GB RAM + 8GB VRAM
   â€¢ 13B+ models: 32GB RAM + 16GB VRAM
    """)

    print_section("7. Quick Reference - Example Queries")

    print("""
âœ… GOOD Examples:

Screen Analysis:
  "çœ‹çœ‹å±å¹•ä¸Šæœ‰ä»€ä¹ˆ"
  "å½“å‰æ˜¾ç¤ºçš„æ˜¯ä»€ä¹ˆåº”ç”¨"
  "æè¿°å±å¹•ä¸Šçš„å†…å®¹"

Search & Find:
  "æœç´¢Pythonä»£ç "
  "æŸ¥æ‰¾å…³äºasyncioçš„å†…å®¹"
  "æ‰¾åˆ°ä»Šå¤©å½•åˆ¶çš„é”™è¯¯ä¿¡æ¯"

Simple Summaries:
  "æ€»ç»“ä»Šå¤©çš„ä»£ç å·¥ä½œ"
  "æ¦‚æ‹¬æœ€è¿‘5æ¡å½•åˆ¶çš„å†…å®¹"

âŒ AVOID Examples:

Too Complex:
  "åˆ†æè¿‡å»ä¸€å‘¨çš„å·¥ä½œæ¨¡å¼ï¼Œè¯†åˆ«æ•ˆç‡ç“¶é¢ˆï¼Œ
   æä¾›è¯¦ç»†çš„æ”¹è¿›å»ºè®®ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š"

Too Vague:
  "å‘Šè¯‰æˆ‘ä¸€äº›æœ‰ç”¨çš„ä¸œè¥¿"

Too Long:
  "æœç´¢ä»ä¸€æœˆä¸€æ—¥åˆ°ç°åœ¨æ‰€æœ‰åŒ…å«ä»¥ä¸‹å…³é”®è¯çš„å†…å®¹ï¼š
   [100+ keywords]ï¼Œå¹¶æ€»ç»“æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†ä¿¡æ¯..."
    """)

    print_section("8. Configuration Tuning")

    print("""
ğŸ”§ Adjust these in agent_executor_v2.py:

# For faster responses (less accurate):
self.max_tokens = 200  # Reduce output length
self.temperature = 0.4  # More focused

# For better quality (slower):
self.max_tokens = 400  # More detailed
self.temperature = 0.7  # More creative

# For consistency:
self.temperature = 0.3  # Very deterministic

# Vision model settings:
"num_predict": 200-400  # Balance speed vs detail
"temperature": 0.6-0.8  # Higher for more descriptive
    """)

    print_header("Ready to Optimize?")

    print("""
ğŸ“ Key takeaways:

1. Keep it simple - small models do best with clear, direct tasks
2. Be patient - local inference is slower than cloud APIs
3. Manage expectations - 3B models have limitations
4. Use appropriate workflows - match task to model capability
5. Iterate - refine prompts based on results

ğŸš€ For advanced users:
   - Edit agent_executor_v2.py for custom behavior
   - Adjust token budgets for your hardware
   - Experiment with different prompts
   - Consider upgrading to larger models if needed

ğŸ’¬ Need help?
   - Check README.md for basic usage
   - Review test_system_comprehensive.py for diagnostics
   - Report issues at github.com/smileformylove/MemScreen

Happy local AI computing! ğŸ‰
    """)

    print("\n" + "=" * 70)
    print(" Guide Complete")
    print("=" * 70 + "\n")


if __name__ == "__main__":
    main()
