#!/usr/bin/env python3
"""
æµ‹è¯•æ€§èƒ½å’Œè¯­æ°”ä¼˜åŒ–æ•ˆæœ

è¿è¡Œæ­¤è„šæœ¬éªŒè¯ä¼˜åŒ–åçš„é…ç½®æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import sys
import time
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


def test_performance_config():
    """æµ‹è¯•æ€§èƒ½é…ç½®"""
    print("\n" + "="*60)
    print("ğŸ“Š æµ‹è¯•æ€§èƒ½é…ç½®")
    print("="*60)

    from memscreen.llm.performance_config import get_optimizer, ModelPerformanceConfig

    # æµ‹è¯•é»˜è®¤é…ç½®
    config = ModelPerformanceConfig()
    print(f"\né»˜è®¤é…ç½®:")
    print(f"  - temperature_chat: {config.temperature_chat} (ç›®æ ‡: 0.45)")
    print(f"  - max_tokens_chat: {config.max_tokens_chat} (ç›®æ ‡: 384)")
    print(f"  - top_p: {config.top_p} (ç›®æ ‡: 0.85)")
    print(f"  - top_k: {config.top_k} (ç›®æ ‡: 25)")

    # æµ‹è¯•ä¼˜åŒ–å™¨
    optimizer = get_optimizer()
    print(f"\nâœ… PerformanceOptimizer åˆå§‹åŒ–æˆåŠŸ")

    # æµ‹è¯•ä¸åŒåœºæ™¯å‚æ•°
    scenarios = ["chat", "chat_fast", "vision", "summary", "search"]
    print(f"\nåœºæ™¯å‚æ•°:")
    for scenario in scenarios:
        params = optimizer.get_optimized_params(scenario)
        print(f"\n  {scenario}:")
        print(f"    - æ¨¡å‹: {params['model']}")
        print(f"    - num_predict: {params['num_predict']}")
        print(f"    - temperature: {params['temperature']}")

    return True


def test_ollama_config():
    """æµ‹è¯• Ollama é…ç½®"""
    print("\n" + "="*60)
    print("ğŸ”§ æµ‹è¯• Ollama é…ç½®")
    print("="*60)

    from memscreen.llm.ollama import OllamaConfig

    config = OllamaConfig()
    print(f"\né»˜è®¤é…ç½®:")
    print(f"  - temperature: {config.temperature} (ç›®æ ‡: 0.45)")
    print(f"  - max_tokens: {config.max_tokens} (ç›®æ ‡: 384)")
    print(f"  - top_p: {config.top_p} (ç›®æ ‡: 0.85)")
    print(f"  - top_k: {config.top_k} (ç›®æ ‡: 25)")

    # éªŒè¯ä¼˜åŒ–å€¼
    assert config.temperature == 0.45, f"temperature åº”è¯¥æ˜¯ 0.45ï¼Œå®é™…æ˜¯ {config.temperature}"
    assert config.max_tokens == 384, f"max_tokens åº”è¯¥æ˜¯ 384ï¼Œå®é™…æ˜¯ {config.max_tokens}"
    assert config.top_p == 0.85, f"top_p åº”è¯¥æ˜¯ 0.85ï¼Œå®é™…æ˜¯ {config.top_p}"
    assert config.top_k == 25, f"top_k åº”è¯¥æ˜¯ 25ï¼Œå®é™…æ˜¯ {config.top_k}"

    print(f"\nâœ… æ‰€æœ‰é…ç½®å€¼éªŒè¯é€šè¿‡")
    return True


def test_prompts():
    """æµ‹è¯• Prompts"""
    print("\n" + "="*60)
    print("ğŸ’¬ æµ‹è¯• Prompts")
    print("="*60)

    from memscreen.prompts import MEMORY_ANSWER_PROMPT, FACT_RETRIEVAL_PROMPT

    print(f"\nMEMORY_ANSWER_PROMPT é•¿åº¦: {len(MEMORY_ANSWER_PROMPT)} å­—ç¬¦")
    print(f"  é¢„æœŸ: çº¦ 800-1000 å­—ç¬¦ï¼ˆç®€åŒ–åï¼‰")
    assert len(MEMORY_ANSWER_PROMPT) < 1500, "Prompt åº”è¯¥æ›´ç®€æ´äº†"
    assert "ç®€æ´ç›´æ¥" in MEMORY_ANSWER_PROMPT, "åº”è¯¥åŒ…å«ä¸­æ–‡æŒ‡ä»¤"
    assert "å‹å¥½è‡ªç„¶" in MEMORY_ANSWER_PROMPT, "åº”è¯¥å¼ºè°ƒå‹å¥½è¯­æ°”"
    print(f"  âœ… Prompt ç®€æ´ä¸”ä½¿ç”¨ä¸­æ–‡")

    print(f"\nFACT_RETRIEVAL_PROMPT é•¿åº¦: {len(FACT_RETRIEVAL_PROMPT)} å­—ç¬¦")
    print(f"  é¢„æœŸ: çº¦ 600-800 å­—ç¬¦ï¼ˆç®€åŒ–åï¼‰")
    assert len(FACT_RETRIEVAL_PROMPT) < 1000, "Prompt åº”è¯¥æ›´ç®€æ´äº†"
    print(f"  âœ… Prompt å·²ç®€åŒ–")

    return True


def test_llm_response():
    """æµ‹è¯• LLM å“åº”é€Ÿåº¦ï¼ˆéœ€è¦ Ollama è¿è¡Œï¼‰"""
    print("\n" + "="*60)
    print("ğŸ¤– æµ‹è¯• LLM å“åº”é€Ÿåº¦")
    print("="*60)

    try:
        from memscreen.llm.ollama import OllamaLLM

        # æ£€æŸ¥ Ollama æ˜¯å¦è¿è¡Œ
        import requests
        try:
            requests.get("http://127.0.0.1:11434/api/tags", timeout=2)
        except:
            print("\nâš ï¸  Ollama æœªè¿è¡Œï¼Œè·³è¿‡å®é™…å“åº”æµ‹è¯•")
            print("   å¯åŠ¨ Ollama: ollama serve")
            return True

        # æµ‹è¯•é»˜è®¤é…ç½®å“åº”
        llm = OllamaLLM()
        messages = [{"role": "user", "content": "ä½ å¥½"}]

        print(f"\næµ‹è¯•ç®€å•æŸ¥è¯¢...")
        start = time.time()
        response = llm.generate_response(messages)
        duration = time.time() - start

        print(f"  å“åº”: {response[:50]}...")
        print(f"  ç”¨æ—¶: {duration:.2f}s")
        print(f"  ç›®æ ‡: < 3s")

        if duration < 3:
            print(f"  âœ… å“åº”é€Ÿåº¦è‰¯å¥½")
        else:
            print(f"  âš ï¸  å“åº”è¾ƒæ…¢ï¼Œå¯èƒ½éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")

        return True

    except Exception as e:
        print(f"\nâš ï¸  æµ‹è¯•å¤±è´¥: {e}")
        return True


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*60)
    print("ğŸ§ª MemScreen ä¼˜åŒ–éªŒè¯æµ‹è¯•")
    print("="*60)

    tests = [
        ("æ€§èƒ½é…ç½®", test_performance_config),
        ("Ollama é…ç½®", test_ollama_config),
        ("Prompts", test_prompts),
        ("LLM å“åº”", test_llm_response),
    ]

    results = []
    for name, test_func in tests:
        try:
            result = test_func()
            results.append((name, result))
        except Exception as e:
            print(f"\nâŒ {name} æµ‹è¯•å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            results.append((name, False))

    # æ€»ç»“
    print("\n" + "="*60)
    print("ğŸ“‹ æµ‹è¯•æ€»ç»“")
    print("="*60)

    for name, result in results:
        status = "âœ… é€šè¿‡" if result else "âŒ å¤±è´¥"
        print(f"  {status} - {name}")

    all_passed = all(result for _, result in results)

    if all_passed:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ä¼˜åŒ–é…ç½®æ­£å¸¸å·¥ä½œã€‚")
        print("\nä¸‹ä¸€æ­¥:")
        print("  1. å¯åŠ¨åº”ç”¨æµ‹è¯•å®é™…æ•ˆæœ")
        print("  2. è§‚å¯Ÿå“åº”é€Ÿåº¦å’Œå›å¤è´¨é‡")
        print("  3. æ ¹æ®éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´å‚æ•°")
    else:
        print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚")

    print("\n" + "="*60 + "\n")

    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())
