# vLLM Inference Backend

vLLM is a high-performance inference engine optimized for production workloads with PagedAttention for efficient memory management.

## üöÄ Why vLLM?

### Performance Benefits

- **PagedAttention** ‚Äî Efficient memory management like OS paging
- **Continuous Batching** ‚Äî Higher throughput for concurrent requests
- **Tensor Parallelism** ‚Äî Distribute models across multiple GPUs
- **OpenAI-Compatible API** ‚Äî Drop-in replacement for cloud APIs
- **Production-Ready** ‚Äî Used by enterprises for inference at scale

### When to Use vLLM

**Use vLLM if you:**
- Have a dedicated NVIDIA GPU (12GB+ VRAM)
- Need higher throughput for concurrent requests
- Want to use larger models (7B parameters+)
- Are deploying in production environments
- Need lower latency for real-time applications

**Use Ollama if you:**
- Are developing on a Mac without GPU
- Want the easiest setup process
- Have limited hardware resources
- Are just getting started

## üìã Quick Start

### 1. Install vLLM

```bash
# Option 1: Install as optional dependency
pip install 'memscreen[vllm]'

# Option 2: Install directly
pip install vllm>=0.6.0 openai>=1.0.0
```

### 2. Start vLLM Server

**Using Docker (Recommended):**

```bash
# Start vLLM server with default model (Qwen2.5-7B-Instruct)
docker-compose -f docker-compose.vllm.yml up -d

# View logs
docker-compose -f docker-compose.vllm.yml logs -f

# Check server health
curl http://localhost:8000/health
```

**Using Python (Offline Mode):**

```bash
# Run vLLM directly (no server needed)
python -c "
from vllm import LLM, SamplingParams
llm = LLM(model='Qwen/Qwen2.5-7B-Instruct')
# Ready to use
"
```

### 3. Configure MemScreen

```bash
# Set environment variables
export MEMSCREEN_LLM_BACKEND=vllm
export MEMSCREEN_VLLM_URL=http://localhost:8000

# Optional: Customize models
export MEMSCREEN_VLLM_LLM_MODEL=Qwen/Qwen2.5-7B-Instruct
export MEMSCREEN_VLLM_VISION_MODEL=Qwen/Qwen2-VL-7B-Instruct
export MEMSCREEN_VLLM_EMBEDDING_MODEL=intfloat/e5-mistral-7b-instruct

# Start MemScreen
python start.py
```

## üîß Configuration

### Environment Variables

| Variable | Default | Description |
|----------|---------|-------------|
| `MEMSCREEN_LLM_BACKEND` | `ollama` | Backend to use (`ollama` or `vllm`) |
| `MEMSCREEN_VLLM_URL` | `http://localhost:8000` | vLLM server URL |
| `MEMSCREEN_VLLM_LLM_MODEL` | `Qwen/Qwen2.5-7B-Instruct` | LLM model name |
| `MEMSCREEN_VLLM_VISION_MODEL` | `Qwen/Qwen2-VL-7B-Instruct` | Vision model name |
| `MEMSCREEN_VLLM_EMBEDDING_MODEL` | `intfloat/e5-mistral-7b-instruct` | Embedding model name |

### Configuration File

You can also configure vLLM via `config.yaml`:

```yaml
llm:
  backend: vllm

vllm:
  base_url: http://localhost:8000
  llm_model: Qwen/Qwen2.5-7B-Instruct
  vision_model: Qwen/Qwen2-VL-7B-Instruct
  embedding_model: intfloat/e5-mistral-7b-instruct
  use_offline_mode: false
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.9
```

## üê≥ Docker Deployment

### Single GPU

```yaml
# docker-compose.vllm.yml
services:
  vllm:
    image: vllm/vllm-openai:latest
    ports:
      - "8000:8000"
    environment:
      - MODEL=Qwen/Qwen2.5-7B-Instruct
      - GPU_MEMORY_UTILIZATION=0.9
      - TENSOR_PARALLEL_SIZE=1
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
```

### Multi-GPU (Tensor Parallelism)

```bash
# Use 4 GPUs for faster inference
export TENSOR_PARALLEL_SIZE=4
docker-compose -f docker-compose.vllm.yml up -d
```

### Custom Model

```bash
# Use a different model
export MODEL=meta-llama/Llama-3.1-8B-Instruct
docker-compose -f docker-compose.vllm.yml up -d
```

## üéØ Supported Models

### Language Models

- **Qwen/Qwen2.5-7B-Instruct** (default) ‚Äî Balanced performance
- **Qwen/Qwen2.5-14B-Instruct** ‚Äî Higher quality
- **meta-llama/Llama-3.1-8B-Instruct** ‚Äî Popular choice
- **mistralai/Mistral-7B-Instruct-v0.3** ‚Äî Fast and efficient

### Vision Models

- **Qwen/Qwen2-VL-7B-Instruct** (default) ‚Äî Best vision-language model
- **Qwen/Qwen2-VL-2B-Instruct** ‚Äî Faster, lower quality

### Embedding Models

- **intfloat/e5-mistral-7b-instruct** (default) ‚Äî High quality
- **BAAI/bge-m3** ‚Äî Multilingual support

## üìä Performance Comparison

Benchmark results on NVIDIA A100 (40GB):

| Metric | Ollama (3B) | vLLM (7B) | Improvement |
|--------|-------------|-----------|-------------|
| **Latency (ms)** | 450 | 120 | 3.75x faster |
| **Throughput (tok/s)** | 25 | 180 | 7.2x higher |
| **Concurrent Requests** | 1 | 10+ | 10x+ |
| **Quality** | Good | Excellent | Better reasoning |

## üîç Troubleshooting

### 1. Out of Memory Error

**Problem:** `CUDA out of memory`

**Solutions:**
```bash
# Reduce GPU memory utilization
export GPU_MEMORY_UTILIZATION=0.7

# Use smaller model
export MODEL=Qwen/Qwen2.5-3B-Instruct

# Increase tensor parallelism (use more GPUs)
export TENSOR_PARALLEL_SIZE=2
```

### 2. Connection Error

**Problem:** `Connection refused` when connecting to vLLM server

**Solutions:**
```bash
# Check if server is running
curl http://localhost:8000/health

# Check Docker logs
docker logs memscreen-vllm

# Verify port is not in use
lsof -i :8000
```

### 3. Model Download Slow

**Problem:** Model download takes too long

**Solutions:**
```bash
# Pre-download models with Hugging Face CLI
pip install huggingface_hub
huggingface-cli download Qwen/Qwen2.5-7B-Instruct

# Or use model cache volume in Docker
docker volume create vllm_models
```

## üìö Advanced Usage

### Offline Mode (Batch Inference)

For batch processing without server overhead:

```python
from memscreen.llm.factory import LlmFactory
from memscreen.config import MemScreenConfig

config = MemScreenConfig()
config._config["vllm"]["use_offline_mode"] = True

# Create LLM in offline mode
llm = LlmFactory.create("vllm", config=config.get_llm_config("vllm"))

# Batch inference
responses = llm.generate_response([
    [{"role": "user", "content": "Hello"}],
    [{"role": "user", "content": "How are you?"}],
])
```

### Custom Sampling Parameters

```python
llm_config = {
    "provider": "vllm",
    "config": {
        "model": "Qwen/Qwen2.5-7B-Instruct",
        "temperature": 0.7,
        "top_p": 0.9,
        "max_tokens": 2000,
    }
}

llm = LlmFactory.create("vllm", config=llm_config)
```

## üîÑ Migration from Ollama

Switching from Ollama to vLLM is simple:

```bash
# 1. Stop using Ollama
# (No need to uninstall, just switch backend)

# 2. Install vLLM
pip install 'memscreen[vllm]'

# 3. Start vLLM server
docker-compose -f docker-compose.vllm.yml up -d

# 4. Switch backend
export MEMSCREEN_LLM_BACKEND=vllm

# 5. Start MemScreen
python start.py
```

**No code changes required!** The abstraction layer handles everything.

## üìñ References

- [vLLM Official Documentation](https://docs.vllm.ai/)
- [vLLM GitHub](https://github.com/vllm-project/vllm)
- [PagedAttention Paper](https://arxiv.org/abs/2309.06180)
- [Model Zoo](https://docs.vllm.ai/en/latest/models/supported_models.html)

## ü§ù Contributing

If you encounter issues or have suggestions:
- Submit [GitHub Issues](https://github.com/smileformylove/MemScreen/issues)
- Join [Discussions](https://github.com/smileformylove/MemScreen/discussions)
